# Ubuntu System Management



### ğŸŒ± mlocate

How to update database: [HERE](https://askubuntu.com/questions/520963/how-come-the-locate-command-doesnt-find-obvious-files)
```
sudo updatedb
```
Note: **NOT** `sudo mlocate updatedb`!

### ğŸŒ± Auto SSH Keyed Login for Remote SSH Sessions

- Setup quick alias for ssh.
- ">" denotes shell commands on local,
- "$" denotes shell commands on remote.
```
> mkdir -p ~/.ssh
> vi ~/.ssh/config

Host some-remote-alias    # You can also call it something else. This is just an alias for the server
    HostName <hostname>   # The hostname such as remote.domain.com
    Port     <port>       # The port. If omitted, defaults to 22
    User     <username>   # Your unix account username
```
- After this, `ssh some-remote-alias` could be used as a shortcut to the full command `ssh -p port username@hostname`. 
- Setup ssh keys on local and remote.
```
# -t ed25519 to generate ed25519 keys instead of rsa keys. 
> ssh-keygen -t ed25519
> cat ~/.ssh/id_xxx.pub
# Copy the contents
> ssh some-remote-alias
# Use account password to log in
```
```
$ mkdir -p ~/.ssh
# If the key was generated on windows, make sure to escape the backslash by adding double backslash
$ echo "<COPIED PUB KEY>" >> ~/.ssh/authorized_keys
$ chmod go-w ~
$ chmod 700 ~/.ssh
$ chmod 600 ~/.ssh/authorized_keys
```
- To verify that authentication is working, open a new terminal session and try SSHing in again. You should be asked for the key passphrase if you configured one, not your unix account password.
```
> ssh -v some-remote-alias
# or the alias you used when updating .ssh/config
# You will be asked for the passphrase if your ssh key has one. Enter it when asked.
# Check for the following 2 lines of log to confirm the usage of your key pair (signature and length might vary)
debug1: Offering public key: xxxxxxxx
debug1: Server accepts key: xxxxxxxx
```

### ğŸŒ± Configure SSH Server

```
sudo ssh-keygen -A
sudo apt install openssh-server
dpkg -l | grep ssh  # åº”è¯¥çœ‹åˆ° openssh-server
ps -e | grep ssh  # åº”è¯¥çœ‹åˆ° sshd
```

- å¦‚æœçœ‹åˆ°`sshd`é‚£è¯´æ˜`ssh-server`å·²ç»å¯åŠ¨äº†ã€‚
- å¦‚æœæ²¡æœ‰åˆ™å¯ä»¥è¿™æ ·å¯åŠ¨ï¼š

```
sudo /etc/init.d/ssh stop
sudo /etc/init.d/ssh start

sudo service ssh --full-restart
```

- é…ç½®ç›¸å…³ï¼š
    - `ssh-server`é…ç½®æ–‡ä»¶ä½äº`/etc/ssh/sshd_config`ï¼Œåœ¨è¿™é‡Œå¯ä»¥å®šä¹‰`SSH`çš„æœåŠ¡ç«¯å£ï¼Œé»˜è®¤ç«¯å£æ˜¯`22`ï¼Œä½ å¯ä»¥è‡ªå·±å®šä¹‰æˆå…¶ä»–ç«¯å£å·ï¼Œå¦‚`222`ï¼›
    - æˆ–æŠŠé…ç½®æ–‡ä»¶ä¸­çš„`PermitRootLogin without-password`æ³¨é‡Šæ‰ï¼Œå†å¢åŠ ä¸€å¥`PermitRootLogin yes`ï¼Œç„¶åé‡å¯`SSH`æœåŠ¡ã€‚

- æ­¤æ—¶å·²ç»å¯ä»¥`ssh`ç™»å½•åŒæ—¶æ”¯æŒ`sftp`ã€‚

- sshå…å¯†ç ç™»å½•ï¼šå°†éœ€è¦å…å¯†ç çš„æœºå™¨çš„`ssh`å…¬é’¥`id_rsa.pub`æ‹·è´è‡³`${HOME}/.ssh/authorized_ssh`ã€‚

#### `WSL`

```
sudo ssh-keygen -A
sudo apt install openssh-server
sudo vi /etc/ssh/sshd_config

# change following things:
# 1. remove the comment on `port 22` (line 15)
# 2. PasswordAuthentication no => PasswordAuthentication yes (line 58)

# sudo ssh-keygen -A
sudo service ssh --full-restart

# e.g. in Windows Powershell
ssh ax@127.0.0.1 -p ...
```

### ğŸŒ± é…ç½®`fail2ban`

```
sudo apt install fail2ban
sudo vi /etc/fail2ban/jail.local 

[sshd]
enabled = true
banaction = iptables-multiport
maxretry = 5
findtime = 900  # ban ip if fail for 5 times in 900 seconds
bantime = 1500  # ban ip for 1500 seconds
port = 130      # ssh port 130. do not need to specify if using port 22

sudo service fail2ban restart
```

### ğŸŒ± `su`

- æ ¼å¼ï¼š
```bash
su [OPTIONS] [-] [USER [ARGUMENT...]]
```
- OPTIONS
```
When called without arguments, su defaults to running an interactive shell as root.

-, -l, --login
      Start the shell as a login shell with an environment similar to a real login:

          o      clears all the environment variables except TERM and variables specified by --whitelist-environment

          o      initializes the environment variables HOME, SHELL, USER, LOGNAME, and PATH

          o      changes to the target user's home directory

          o      sets argv[0] of the shell to '-' in order to make the shell a login shell

-m, -p, --preserve-environment
      Preserve the entire environment, i.e. it does not set HOME, SHELL, USER nor LOGNAME.  
      This option is ignored if the option --login is specified.
```
### ğŸŒ± æ–°å»ºç”¨æˆ·

```
sudo adduser <user>  # æŒ‰ç…§æç¤ºæ¥ï¼Œä¸€èˆ¬é»˜è®¤å°±è¡Œ
```

- æŸ¥çœ‹ç”¨æˆ·ï¼š`cat /etc/passwd`
- åˆ é™¤ç”¨æˆ·ï¼š`deluser`
- è®¾ç½®`root`å¯†ç ï¼š`sudo passwd root`
- æ·»åŠ `sudoers`ï¼š

```
sudo su - 
visudo

# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) ALL
ax      ALL=(ALL:ALL) ALL
```

- æ·»åŠ å…å¯†`sudoers`ï¼š

```
sudo su - 
visudo

# Allow members of group sudo to execute any command
%sudo   ALL=(ALL:ALL) ALL
ax      ALL=(ALL:ALL) NOPASSWD : ALL
```

#### åˆ‡æ¢ç”¨æˆ·

- `su`ä»¥åŠ`su -`ï¼Œ`su -l`åé¢ä¸åŠ ç”¨æˆ·ï¼Œåˆ™é»˜è®¤åˆ‡åˆ° `root`
- `su`æ˜¯ä¸æ”¹å˜å½“å‰å˜é‡ï¼ˆonly `.bashrc` will be sourcedï¼‰
- `su -`æ˜¯`su -l`çš„ç®€å†™ï¼Œâ€œæ¨¡æ‹Ÿäº†ä¸€æ¬¡ç™»å½•ï¼ˆloginï¼‰â€ï¼Œæ”¹å˜ä¸ºåˆ‡æ¢åˆ°ç”¨æˆ·çš„å˜é‡ï¼ˆexpericene a login process, usually `.bash_profile` and `.bashrc` will be sourcedï¼‰
- ä¹Ÿå°±æ˜¯è¯´`su`åªèƒ½è·å¾—`root`çš„æ‰§è¡Œæƒé™ï¼Œä¸èƒ½è·å¾—ç¯å¢ƒå˜é‡ï¼›è€Œ`su -`æ˜¯åˆ‡æ¢åˆ°`root`å¹¶è·å¾—`root`çš„ç¯å¢ƒå˜é‡åŠæ‰§è¡Œæƒé™

### ğŸŒ± ä¼ªåˆ†å¸ƒå¼`hadoop`

`hadoop`éœ€è¦`jdk 8`ã€‚
æ­£å¼éƒ¨ç½²åº”å½“ä¸º`hadoop`å•ç‹¬åˆ›å»ºè´¦å·ï¼Œå•æœºä¼ªåˆ†å¸ƒå¼é…ç½®ç€ç©å„¿ä¸€ä¸‹å°±ä¸ç”¨å•¦ã€‚
çœŸæ­£çš„åˆ†å¸ƒå¼å®‰è£…å¯ä»¥çœ‹`https://zhuanlan.zhihu.com/p/77938727`ã€‚
å®‰è£…æµç¨‹å¦‚ä¸‹ï¼š

- é…ç½®`ssh`ä»¥åŠå…å¯†ç™»å½•ï¼ˆå‚è€ƒæœ¬æ–‡ä»¶å¼€å¤´éƒ¨åˆ†ï¼‰ï¼›
- é…ç¯å¢ƒå˜é‡ï¼š
```
cd ~/Downloads
wget https://archive.apache.org/dist/hadoop/core/hadoop-3.2.0/hadoop-3.2.0.tar.gz
tar -zxvf hadoop-3.2.0.tar.gz
mv hadoop-3.2.0 /home/ax/opt/

gedit ~/.bashrc

# hadoop
export HADOOP_HOME=/home/ax/opt/hadoop-3.2.0
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export YARN_HOME=${HADOOP_HOME}
export PATH=${PATH}:${HADOOP_HOME}/bin

source ~/.bashrc
hadoop version
```
- å•ç‹¬é…ç½®`${HADOOP_CONF_DIR}/hadoop-env.sh`ï¼š
```
gedit ${HADOOP_CONF_DIR}/hadoop-env.sh

# jvm
# in case hadoop does not recognize ${JAVA_HOME} exported in `/etc/environment`
# just let it happy
export JAVA_HOME=/opt/jvm/jdk
```
- ä¿®æ”¹4ä¸ªé…ç½®æ–‡ä»¶çš„`<configuration></configuration>`åŸŸï¼š
    - `core-site.xml`ï¼Œ`10497`æ˜¯`h`å’Œ`a`çš„`ASCII`ç ï¼Œå…¶å®éšä¾¿è°éƒ½è¡Œï¼š
    ```
    gedit ${HADOOP_CONF_DIR}/core-site.xml
    
    <configuration>
    <property>
      <name>fs.defaultFS</name>
      <value>hdfs://localhost:10497</value>
    </property>
    </configuration>
    ```
    - `hdfs-site.xml`ï¼Œç¬¬ä¸€ä¸ªæ˜¯`dfs`çš„å¤‡ä»½æ•°ç›®ï¼Œå•æœºç”¨`1`å°±è¡Œï¼Œåé¢ä¸¤ä¸ªæ˜¯`namenode`å’Œ`datanode`çš„ç›®å½•ï¼š
    ```
    gedit ${HADOOP_CONF_DIR}/hdfs-site.xml
    
    <configuration>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>file:///home/ax/var/hadoop/hdfs/datanode</value>
    </property>
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>file:///home/ax/var/hadoop/hdfs/namenode</value>
    </property>
    </configuration>
    ```
    - `mapred-site.xml`ï¼š
    ```
    gedit ${HADOOP_CONF_DIR}/mapred-site.xml
    
    <configuration>
      <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
      </property>
    </configuration>
    ```
    - `yarn-site.xml`ï¼š
    ```
    gedit ${HADOOP_CONF_DIR}/yarn-site.xml
    
    <configuration>
      <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
    </configuration>
    ```
- æ ¼å¼åŒ–`HDFS`ï¼š
```
hadoop namenode -format
```
- å¯åŠ¨æˆ–å…³é—­ï¼š
```
${HADOOP_HOME}/sbin/start-all.sh
${HADOOP_HOME}/sbin/stop-all.sh
```
- æµ‹è¯•`hdfs`ï¼Œä¸æŠ¥é”™ï¼š
    - æˆ–è€…æµè§ˆå™¨ä¸­æ‰“å¼€`http://localhost:9870`ï¼Œè¿™ä¸ªæ˜¯`hadoop`çŠ¶æ€é¡µã€‚
```
hdfs dfs -ls /
```
    
## ğŸŒ± `spark`

```
cd ~/Downloads
wget http://apache.mirrors.hoobly.com/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz
tar -xzvf spark-3.0.0-preview2-bin-hadoop3.2.tgz  
mv spark-3.0.0-preview2-bin-hadoop3.2 ~/opt/

gedit ~/.bashrc

# spark
export SPARK_HOME=/home/ax/opt/spark-3.0.0-preview2-bin-hadoop3.2
export PATH=${SPARK_HOME}/bin:${PATH}
```

```
cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
gedit $SPARK_HOME/conf/spark-env.sh

export SPARK_MASTER_IP=master
export SPARK_WORKER_MEMORY=4G
```

```
$SPARK_HOME/sbin/start-all.sh
jps
$SPARK_HOME/bin/run-example SparkPi
çœ‹çœ‹ä½ æœ‰æ²¡æœ‰å¾—åˆ°:
Pi is roughly 3.14716
```
